import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.Transformer_EncDec import Encoder, EncoderLayer
from layers.SelfAttention_Family import FullAttention, AttentionLayer
from layers.Embed import DataEmbedding_inverted
from layers.star import STAR
from layers.star_block import STARBlock
from layers.patch_embed import PatchEmbed1D
# from modules.ms_patch import MSPatchEmbed
# from modules.ms_router import MSPatchRouter
# from modules.patch_filter import PatchFilter
# from modules.patch_mixer import PatchMixer
from model.modules.cac_block import CACBlock
from model.modules.gated_fusion import GatedFusion
from model.modules.channel_mixer import ChannelMixerHead
from model.modules.frequency_enhance import FrequencyEnhance
import numpy as np


class Model(nn.Module):
    """
    Paper link: https://arxiv.org/abs/2310.06625
    """

    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm
        self.gate_init_bias = float(getattr(configs, 'gate_init_bias', -2.0))
        self.use_gated_residual_flag = bool(getattr(configs, 'use_gated_residual', 0))
        self.gate_attn_enabled = bool(getattr(configs, 'gate_attn', 1))
        self.gate_ffn_enabled = bool(getattr(configs, 'gate_ffn', 1))
        gated_layers_spec = str(getattr(configs, 'gated_layers', '')).strip()
        if gated_layers_spec:
            layers_set = set()
            for tok in gated_layers_spec.split(','):
                tok = tok.strip()
                if not tok:
                    continue
                try:
                    layers_set.add(int(tok))
                except ValueError:
                    continue
            self.gated_layers = sorted(layers_set)
        else:
            self.gated_layers = None
        self.log_gate_stats = bool(getattr(configs, 'log_gate_stats', 0))
        self.gate_log_interval = int(getattr(configs, 'gate_log_interval', 200))
        
        # STAR and Patch Embedding parameters
        self.use_star = getattr(configs, 'use_star', 0)
        self.use_patch_embed = getattr(configs, 'use_patch_embed', 0)
        self.patch_len = getattr(configs, 'patch_len', 16)
        self.patch_stride = getattr(configs, 'patch_stride', 8)
        self.patch_norm = getattr(configs, 'patch_norm', 'instance')
        self.patch_pos_emb = getattr(configs, 'patch_pos_emb', 'sincos')
        self.use_ci = getattr(configs, 'use_ci', 1)
        self.use_star_fusion_gate = bool(getattr(configs, 'use_star_fusion_gate', 1))
        self.use_cac_fusion_gate = bool(getattr(configs, 'use_cac_fusion_gate', 1))
        
        # Initialize STAR (pre) is removed; keep compatibility no-op
        self.star = None
            
        # Initialize Patch Embedding module
        if self.use_patch_embed:
            self.patch_embed = PatchEmbed1D(
                d_model=configs.d_model,
                patch_len=self.patch_len,
                patch_stride=self.patch_stride,
                norm=self.patch_norm,
                pos_emb=self.patch_pos_emb,
                use_ci=bool(self.use_ci)
            )
        else:
            self.patch_embed = None
            
        # Embedding
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,
                                                    configs.dropout)
        self.class_strategy = configs.class_strategy
        # Encoder-only architecture
        encoder_layers = []
        for l in range(configs.e_layers):
            layer_gate_active = self.use_gated_residual_flag
            if layer_gate_active and self.gated_layers is not None:
                layer_gate_active = l in self.gated_layers
            encoder_layers.append(
                EncoderLayer(
                    AttentionLayer(
                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,
                                      output_attention=configs.output_attention), configs.d_model, configs.n_heads),
                    configs.d_model,
                    configs.d_ff,
                    dropout=configs.dropout,
                    activation=configs.activation,
                    use_gated_residual=layer_gate_active,
                    gate_init_bias=self.gate_init_bias,
                    log_gate_stats=self.log_gate_stats,
                    gate_log_interval=self.gate_log_interval,
                    layer_id=l,
                    enable_attn_gate=self.gate_attn_enabled,
                    enable_ffn_gate=self.gate_ffn_enabled
                )
            )
        self.encoder = Encoder(
            encoder_layers,
            norm_layer=torch.nn.LayerNorm(configs.d_model)
        )
        self.projector = nn.Linear(configs.d_model, configs.pred_len, bias=True)

        # STARBlock (post-forecast)
        self.star_block = None
        self.star_fusion = None
        if self.use_star:
            c_out = int(getattr(configs, 'c_out', 1))
            k = int(getattr(configs, 'moving_avg', 25))
            self.star_block = STARBlock(c_out=c_out, k=k, gate_bias=-4.0, use_ln=True)
            if self.use_star_fusion_gate:
                self.star_fusion = GatedFusion(dim=c_out, gate_init_bias=self.gate_init_bias)

        # 单尺度 Patch 嵌入构造器
        def single_embed_ctor(P, S, **kwargs):
            return PatchEmbed1D(
                d_model=configs.d_model,
                patch_len=P,
                patch_stride=S,
                norm=getattr(configs, 'patch_norm', 'instance'),
                pos_emb=getattr(configs, 'patch_pos_emb', 'sincos'),
                use_ci=bool(getattr(configs, 'use_ci', 1))
            )

        # 多尺度 Patch 模块（暂时未使用，注释掉以避免导入错误）
        # self.ms_embed = None
        # self.ms_router = None
        # self.patch_filter = None
        # self.patch_mixer = None
        self._aux_loss = torch.zeros(1)

        # if getattr(configs, 'use_ms_patch', 0):
        #     patch_set = [int(x) for x in getattr(configs, 'ms_patch_set', '8,16,32').split(",")]
        #     stride_ratio = getattr(configs, 'ms_stride_ratio', 0.5)
        #     fuse = getattr(configs, 'ms_fuse', 'gate')
        #     self.ms_embed = MSPatchEmbed(single_embed_ctor, patch_set, stride_ratio, fuse=fuse)

        # if getattr(configs, 'use_ms_router', 0):
        #     patch_set = [int(x) for x in getattr(configs, 'ms_patch_set', '8,16,32').split(",")]
        #     stride_ratio = getattr(configs, 'ms_stride_ratio', 0.5)
        #     hidden = getattr(configs, 'router_hidden', 64)
        #     budget_weight = getattr(configs, 'router_budget', 0.0)
        #     self.ms_router = MSPatchRouter(single_embed_ctor, patch_set, stride_ratio,
        #                                    hidden=hidden, budget_weight=budget_weight)

        # if getattr(configs, 'use_patch_filter', 0):
        #     topk = getattr(configs, 'filter_topk', 8)
        #     season = getattr(configs, 'filter_season', '')
        #     self.patch_filter = PatchFilter(topk=topk, season=season)

        # if getattr(configs, 'use_patch_mixer', 0):
        #     groups = getattr(configs, 'mixer_groups', 8)
        #     dropout = getattr(configs, 'mixer_dropout', 0.1)
        #     self.patch_mixer = PatchMixer(d_model=configs.d_model, groups=groups, dropout=dropout)

        # CAC-Block: 时间轴周期归纳偏置
        self.cac = None
        self.cac_fusion = None
        if getattr(configs, 'use_cac', 0):
            self.cac = CACBlock(
                d_model=configs.d_model,
                kernel_sizes=getattr(configs, 'cac_kernel_sizes', '3,5,7'),
                dilations=getattr(configs, 'cac_dilations', '1,2,4'),
                topk=getattr(configs, 'cac_topk', 5),
                dropout=getattr(configs, 'cac_dropout', 0.1),
            )
            if self.use_cac_fusion_gate:
                self.cac_fusion = GatedFusion(dim=configs.d_model, gate_init_bias=self.gate_init_bias)
            else:
                self.cac_fusion = None
            # print(f"[PS] CAC-Block enabled.")  # 已在下方统一打印

        # =========================
        # [FEM] 频域增强模块（放在 Encoder 之前）
        # =========================
        self.use_freq_enhance = bool(getattr(configs, 'use_freq_enhance', 0))
        if self.use_freq_enhance:
            bands = int(getattr(configs, 'freq_bands', 3))
            freq_init = getattr(configs, 'freq_init', '')
            mix = float(getattr(configs, 'freq_mix', 0.5))
            if freq_init:
                init_tuple = tuple(float(x) for x in freq_init.split(','))
                assert len(init_tuple) == bands, "freq_init length must match freq_bands"
            else:
                init_tuple = (1.0, 1.0, 0.1) if bands == 3 else tuple([1.0 for _ in range(bands)])
            self.freq_enhance = FrequencyEnhance(
                seq_len=self.seq_len, n_bands=bands, init_weights=init_tuple, mix=mix
            )
            # print(f"[PS] FreqEnhance enabled (pre-encoder, L={self.seq_len}, F={self.seq_len//2+1}).")  # 已废弃
        else:
            self.freq_enhance = None

        # ChannelMixer-Head: 变量维跨通道混合
        self.channel_mixer = None
        if getattr(configs, 'use_channel_mixer', 0):
            self.channel_mixer = ChannelMixerHead(
                c_out=configs.c_out,
                hidden_ratio=getattr(configs, 'cm_hidden_ratio', 2.0),
                dropout=getattr(configs, 'cm_dropout', 0.1),
            )
            # print("[PS] ChannelMixer-Head enabled.")  # 已废弃

        # Print module status (只显示当前使用的模块)
        print(f"[PS] STARBlock enabled={int(bool(self.use_star))}")
        gated_enabled = int(self.use_gated_residual_flag)
        if gated_enabled:
            layer_desc = "all" if self.gated_layers is None else ("none" if not self.gated_layers else ",".join(str(idx) for idx in self.gated_layers))
            print(f"[PS] Gated Residual enabled=1 (layers={layer_desc}, attn={int(self.gate_attn_enabled)}, ffn={int(self.gate_ffn_enabled)}, bias={self.gate_init_bias})")
        else:
            print(f"[PS] Gated Residual enabled=0")
        if getattr(configs, 'use_cac', 0):
            print(f"[PS] CAC fusion gate enabled={int(self.cac_fusion is not None)}")
        if self.use_star:
            print(f"[PS] STAR fusion gate enabled={int(self.star_fusion is not None)}")
        if self.log_gate_stats:
            print(f"[PS] Gate stats logging enabled every {self.gate_log_interval} steps.")
        print(f"[PS] CAC enabled={getattr(configs, 'use_cac', 0)} ks={getattr(configs, 'cac_kernel_sizes', '3,5,7')} dil={getattr(configs, 'cac_dilations', '1,2,4')} topk={getattr(configs, 'cac_topk', 5)}")
        # 已废弃的模块（不再显示）
        # print(f"[PS] patch={self.use_patch_embed}, P={self.patch_len}, S={self.patch_stride}, norm={self.patch_norm}, pos={self.patch_pos_emb}, CI={self.use_ci}")
        # print(f"[PS] FreqEnhance enabled={int(self.use_freq_enhance)} bands={getattr(configs, 'freq_bands', 3)} mix={getattr(configs, 'freq_mix', 0.5)}")
        # print(f"[PS] ChannelMixer enabled={getattr(configs, 'use_channel_mixer', 0)} ratio={getattr(configs, 'cm_hidden_ratio', 2.0)}")
        
        # ---- 递归设置所有 FullAttention 的 ASP-Lite 开关 ----
        # 注意：FullAttention 已在文件顶部导入，无需再次导入
        
        def _apply_asp_lite_flags(module: nn.Module):
            """递归设置所有 FullAttention 实例的 QK-Norm 和 Cosine 开关"""
            for m in module.modules():
                if isinstance(m, FullAttention):
                    # 互斥保护：如果同时开启，Cosine 优先
                    use_qk = int(getattr(configs, 'use_qk_norm', 0))
                    use_cos = int(getattr(configs, 'use_cosine_attn', 0))
                    
                    if use_qk and use_cos:
                        print("[PS] Warning: Both QK-Norm and Cosine requested. Cosine takes priority.")
                        use_qk = 0  # 关闭 QK-Norm
                    
                    m.use_qk_norm = bool(use_qk)
                    m.qk_eps = float(getattr(configs, 'qk_eps', 1e-6))
                    m.use_cosine_attn = bool(use_cos)
                    m.cos_scale = float(getattr(configs, 'cos_scale', 1.0))
        
        _apply_asp_lite_flags(self)
        
        # ASP-Lite 状态打印
        asp_qk = int(getattr(configs, 'use_qk_norm', 0))
        asp_cos = int(getattr(configs, 'use_cosine_attn', 0))
        if asp_qk or asp_cos:
            print(f"[PS] ASP-Lite: QK-Norm={asp_qk}, CosineAttn={asp_cos}")

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.use_norm:
            # Normalization from Non-stationary Transformer
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

        # (Removed old pre-STAR path)

        _, _, N = x_enc.shape # B L N
        # B: batch_size;    E: d_model; 
        # L: seq_len;       S: pred_len;
        # N: number of variate (tokens), can also includes covariates

        # Embedding with new patch modules
        attn_mask = None

        # 1) 产出 tokens（多尺度模块已注释，简化为单路径）
        # if self.ms_router is not None:
        #     enc_out = self.ms_router(x_enc, x_mark_enc)     # (B, L*, D)
        #     self._aux_loss = self.ms_router.get_aux_loss()
        # elif self.ms_embed is not None:
        #     enc_out = self.ms_embed(x_enc, x_mark_enc)
        #     self._aux_loss = enc_out.new_zeros(1)
        if self.patch_embed is not None:
            # Use Patch Embedding instead of DataEmbedding
            enc_tokens, _meta = self.patch_embed(x_enc)  # [B, C, D]
            self._aux_loss = enc_tokens.new_zeros(1)
        else:
            # B L N -> B N E                (B L N -> B L E in the vanilla Transformer)
            enc_tokens = self.enc_embedding(x_enc, x_mark_enc)
            self._aux_loss = enc_tokens.new_zeros(1)

        # 2) PatchMixer（已注释）
        # if self.patch_mixer is not None:
        #     enc_tokens = self.patch_mixer(enc_tokens)

        # 3) PatchFilter -> attention mask（已注释）
        # if self.patch_filter is not None:
        #     attn_mask = self.patch_filter(enc_tokens)  # (B,1,L,L)

        # -------- [FEM] 于 Encoder 之前（保留原始频率信息）----------
        if self.freq_enhance is not None:
            enc_tokens = self.freq_enhance(enc_tokens)  # [B, N, D]
        # ---------------------------------------------------------

        # 4) 送入 encoder
        encoder_core, attns = self.encoder(enc_tokens, attn_mask=attn_mask)

        # 5) CAC-Block: 在 encoder 输出后、投影前做周期增强并通过门控融合
        fused_encoder = encoder_core
        if self.cac is not None:
            cac_out = self.cac(encoder_core)
            fused_encoder = self.cac_fusion(encoder_core, cac_out) if self.cac_fusion is not None else cac_out

        # B N E -> B N S -> B S N
        dec_out = self.projector(fused_encoder).permute(0, 2, 1)[:, :, :N]  # filter the covariates

        if self.use_norm:
            # De-Normalization from Non-stationary Transformer
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))

        # 6) ChannelMixer-Head: 在变量维做全局混合（在反归一化后，形状 [B, L, C]）
        if self.channel_mixer is not None:
            dec_out = self.channel_mixer(dec_out)  # [B, L, C]

        # Post-forecast STARBlock refinement + gated融合
        if self.star_block is not None:
            star_out = self.star_block(dec_out)
            dec_out = self.star_fusion(dec_out, star_out) if self.star_fusion is not None else star_out

        return dec_out, attns


    def get_aux_loss(self):
        return getattr(self, "_aux_loss", 0.0)

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        dec_out, attns = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        if self.output_attention:
            return dec_out[:, -self.pred_len:, :], attns
        else:
            return dec_out[:, -self.pred_len:, :]  # [B, L, D]

import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.Transformer_EncDec import Encoder, EncoderLayer
from layers.SelfAttention_Family import FullAttention, AttentionLayer
from layers.Embed import DataEmbedding_inverted
from layers.star import STAR
from layers.star_block import STARBlock
from layers.patch_embed import PatchEmbed1D
# from modules.ms_patch import MSPatchEmbed
# from modules.ms_router import MSPatchRouter
# from modules.patch_filter import PatchFilter
# from modules.patch_mixer import PatchMixer
from model.modules.cac_block import CACBlock
from model.modules.channel_mixer import ChannelMixerHead
from model.modules.frequency_enhance import FrequencyEnhance
import numpy as np


class Model(nn.Module):
    """
    Paper link: https://arxiv.org/abs/2310.06625
    """

    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm
        
        # STAR and Patch Embedding parameters
        self.use_star = getattr(configs, 'use_star', 0)
        self.use_patch_embed = getattr(configs, 'use_patch_embed', 0)
        self.patch_len = getattr(configs, 'patch_len', 16)
        self.patch_stride = getattr(configs, 'patch_stride', 8)
        self.patch_norm = getattr(configs, 'patch_norm', 'instance')
        self.patch_pos_emb = getattr(configs, 'patch_pos_emb', 'sincos')
        self.use_ci = getattr(configs, 'use_ci', 1)
        
        # Initialize STAR (pre) is removed; keep compatibility no-op
        self.star = None
            
        # Initialize Patch Embedding module
        if self.use_patch_embed:
            self.patch_embed = PatchEmbed1D(
                d_model=configs.d_model,
                patch_len=self.patch_len,
                patch_stride=self.patch_stride,
                norm=self.patch_norm,
                pos_emb=self.patch_pos_emb,
                use_ci=bool(self.use_ci)
            )
        else:
            self.patch_embed = None
            
        # Embedding
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,
                                                    configs.dropout)
        self.class_strategy = configs.class_strategy
        # Encoder-only architecture
        self.encoder = Encoder(
            [
                EncoderLayer(
                    AttentionLayer(
                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,
                                      output_attention=configs.output_attention), configs.d_model, configs.n_heads),
                    configs.d_model,
                    configs.d_ff,
                    dropout=configs.dropout,
                    activation=configs.activation,
                    use_gated_residual=bool(getattr(configs, 'use_gated_residual', 0))
                ) for l in range(configs.e_layers)
            ],
            norm_layer=torch.nn.LayerNorm(configs.d_model)
        )
        self.projector = nn.Linear(configs.d_model, configs.pred_len, bias=True)

        # STARBlock (post-forecast)
        if self.use_star:
            c_out = int(getattr(configs, 'c_out', 1))
            k = int(getattr(configs, 'moving_avg', 25))
            self.star_block = STARBlock(c_out=c_out, k=k, gate_bias=-4.0, use_ln=True)
        else:
            self.star_block = None

        # 单尺度 Patch 嵌入构造器
        def single_embed_ctor(P, S, **kwargs):
            return PatchEmbed1D(
                d_model=configs.d_model,
                patch_len=P,
                patch_stride=S,
                norm=getattr(configs, 'patch_norm', 'instance'),
                pos_emb=getattr(configs, 'patch_pos_emb', 'sincos'),
                use_ci=bool(getattr(configs, 'use_ci', 1))
            )

        # 多尺度 Patch 模块（暂时未使用，注释掉以避免导入错误）
        # self.ms_embed = None
        # self.ms_router = None
        # self.patch_filter = None
        # self.patch_mixer = None
        self._aux_loss = torch.zeros(1)

        # if getattr(configs, 'use_ms_patch', 0):
        #     patch_set = [int(x) for x in getattr(configs, 'ms_patch_set', '8,16,32').split(",")]
        #     stride_ratio = getattr(configs, 'ms_stride_ratio', 0.5)
        #     fuse = getattr(configs, 'ms_fuse', 'gate')
        #     self.ms_embed = MSPatchEmbed(single_embed_ctor, patch_set, stride_ratio, fuse=fuse)

        # if getattr(configs, 'use_ms_router', 0):
        #     patch_set = [int(x) for x in getattr(configs, 'ms_patch_set', '8,16,32').split(",")]
        #     stride_ratio = getattr(configs, 'ms_stride_ratio', 0.5)
        #     hidden = getattr(configs, 'router_hidden', 64)
        #     budget_weight = getattr(configs, 'router_budget', 0.0)
        #     self.ms_router = MSPatchRouter(single_embed_ctor, patch_set, stride_ratio,
        #                                    hidden=hidden, budget_weight=budget_weight)

        # if getattr(configs, 'use_patch_filter', 0):
        #     topk = getattr(configs, 'filter_topk', 8)
        #     season = getattr(configs, 'filter_season', '')
        #     self.patch_filter = PatchFilter(topk=topk, season=season)

        # if getattr(configs, 'use_patch_mixer', 0):
        #     groups = getattr(configs, 'mixer_groups', 8)
        #     dropout = getattr(configs, 'mixer_dropout', 0.1)
        #     self.patch_mixer = PatchMixer(d_model=configs.d_model, groups=groups, dropout=dropout)

        # CAC-Block: 时间轴周期归纳偏置
        self.cac = None
        if getattr(configs, 'use_cac', 0):
            self.cac = CACBlock(
                d_model=configs.d_model,
                kernel_sizes=getattr(configs, 'cac_kernel_sizes', '3,5,7'),
                dilations=getattr(configs, 'cac_dilations', '1,2,4'),
                topk=getattr(configs, 'cac_topk', 5),
                dropout=getattr(configs, 'cac_dropout', 0.1),
            )
            # print(f"[PS] CAC-Block enabled.")  # 已在下方统一打印

        # =========================
        # [FEM] 频域增强模块（放在 Encoder 之前）
        # =========================
        self.use_freq_enhance = bool(getattr(configs, 'use_freq_enhance', 0))
        if self.use_freq_enhance:
            bands = int(getattr(configs, 'freq_bands', 3))
            freq_init = getattr(configs, 'freq_init', '')
            mix = float(getattr(configs, 'freq_mix', 0.5))
            if freq_init:
                init_tuple = tuple(float(x) for x in freq_init.split(','))
                assert len(init_tuple) == bands, "freq_init length must match freq_bands"
            else:
                init_tuple = (1.0, 1.0, 0.1) if bands == 3 else tuple([1.0 for _ in range(bands)])
            self.freq_enhance = FrequencyEnhance(
                seq_len=self.seq_len, n_bands=bands, init_weights=init_tuple, mix=mix
            )
            # print(f"[PS] FreqEnhance enabled (pre-encoder, L={self.seq_len}, F={self.seq_len//2+1}).")  # 已废弃
        else:
            self.freq_enhance = None

        # ChannelMixer-Head: 变量维跨通道混合
        self.channel_mixer = None
        if getattr(configs, 'use_channel_mixer', 0):
            self.channel_mixer = ChannelMixerHead(
                c_out=configs.c_out,
                hidden_ratio=getattr(configs, 'cm_hidden_ratio', 2.0),
                dropout=getattr(configs, 'cm_dropout', 0.1),
            )
            # print("[PS] ChannelMixer-Head enabled.")  # 已废弃

        # Print module status (只显示当前使用的模块)
        print(f"[PS] STARBlock enabled={int(bool(self.use_star))}")
        gated_enabled = int(bool(getattr(configs, 'use_gated_residual', 0)))
        if gated_enabled:
            print(f"[PS] Gated Residual enabled=1 (Gateformer原文：bias=False, Kaiming init, gate≈0.5)")
        else:
            print(f"[PS] Gated Residual enabled=0")
        print(f"[PS] CAC enabled={getattr(configs, 'use_cac', 0)} ks={getattr(configs, 'cac_kernel_sizes', '3,5,7')} dil={getattr(configs, 'cac_dilations', '1,2,4')} topk={getattr(configs, 'cac_topk', 5)}")
        # 已废弃的模块（不再显示）
        # print(f"[PS] patch={self.use_patch_embed}, P={self.patch_len}, S={self.patch_stride}, norm={self.patch_norm}, pos={self.patch_pos_emb}, CI={self.use_ci}")
        # print(f"[PS] FreqEnhance enabled={int(self.use_freq_enhance)} bands={getattr(configs, 'freq_bands', 3)} mix={getattr(configs, 'freq_mix', 0.5)}")
        # print(f"[PS] ChannelMixer enabled={getattr(configs, 'use_channel_mixer', 0)} ratio={getattr(configs, 'cm_hidden_ratio', 2.0)}")
        
        # ---- 递归设置所有 FullAttention 的 ASP-Lite 开关 ----
        # 注意：FullAttention 已在文件顶部导入，无需再次导入
        
        def _apply_asp_lite_flags(module: nn.Module):
            """递归设置所有 FullAttention 实例的 QK-Norm 和 Cosine 开关"""
            for m in module.modules():
                if isinstance(m, FullAttention):
                    # 互斥保护：如果同时开启，Cosine 优先
                    use_qk = int(getattr(configs, 'use_qk_norm', 0))
                    use_cos = int(getattr(configs, 'use_cosine_attn', 0))
                    
                    if use_qk and use_cos:
                        print("[PS] Warning: Both QK-Norm and Cosine requested. Cosine takes priority.")
                        use_qk = 0  # 关闭 QK-Norm
                    
                    m.use_qk_norm = bool(use_qk)
                    m.qk_eps = float(getattr(configs, 'qk_eps', 1e-6))
                    m.use_cosine_attn = bool(use_cos)
                    m.cos_scale = float(getattr(configs, 'cos_scale', 1.0))
        
        _apply_asp_lite_flags(self)
        
        # ASP-Lite 状态打印
        asp_qk = int(getattr(configs, 'use_qk_norm', 0))
        asp_cos = int(getattr(configs, 'use_cosine_attn', 0))
        if asp_qk or asp_cos:
            print(f"[PS] ASP-Lite: QK-Norm={asp_qk}, CosineAttn={asp_cos}")

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.use_norm:
            # Normalization from Non-stationary Transformer
            means = x_enc.mean(1, keepdim=True).detach()
            x_enc = x_enc - means
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
            x_enc /= stdev

        # (Removed old pre-STAR path)

        _, _, N = x_enc.shape # B L N
        # B: batch_size;    E: d_model; 
        # L: seq_len;       S: pred_len;
        # N: number of variate (tokens), can also includes covariates

        # Embedding with new patch modules
        attn_mask = None

        # 1) 产出 tokens（多尺度模块已注释，简化为单路径）
        # if self.ms_router is not None:
        #     enc_out = self.ms_router(x_enc, x_mark_enc)     # (B, L*, D)
        #     self._aux_loss = self.ms_router.get_aux_loss()
        # elif self.ms_embed is not None:
        #     enc_out = self.ms_embed(x_enc, x_mark_enc)
        #     self._aux_loss = enc_out.new_zeros(1)
        # elif self.patch_embed is not None:
        if self.patch_embed is not None:
            # Use Patch Embedding instead of DataEmbedding
            enc_out, _meta = self.patch_embed(x_enc)  # [B, C, D]
            self._aux_loss = enc_out.new_zeros(1)
        else:
            # B L N -> B N E                (B L N -> B L E in the vanilla Transformer)
            enc_out = self.enc_embedding(x_enc, x_mark_enc) # covariates (e.g timestamp) can be also embedded as tokens
            self._aux_loss = enc_out.new_zeros(1)

        # 2) PatchMixer（已注释）
        # if self.patch_mixer is not None:
        #     enc_out = self.patch_mixer(enc_out)

        # 3) PatchFilter -> attention mask（已注释）
        # if self.patch_filter is not None:
        #     attn_mask = self.patch_filter(enc_out)  # (B,1,L,L)

        # -------- [FEM] 在 Encoder 之前（原始序列 L=96，完整频率信息）----------
        if self.freq_enhance is not None:
            enc_out = self.freq_enhance(enc_out)  # [B, N, D], N=96, F=49 频率点
        # ---------------------------------------------------------

        # 4) 送入 encoder
        enc_out, attns = self.encoder(enc_out, attn_mask=attn_mask)

        # 5) CAC-Block: 在 encoder 输出后、投影前，对时间轴做周期增强
        if self.cac is not None:
            enc_out = self.cac(enc_out)  # [B, L, D]

        # B N E -> B N S -> B S N 
        dec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :N] # filter the covariates

        if self.use_norm:
            # De-Normalization from Non-stationary Transformer
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))

        # 6) ChannelMixer-Head: 在变量维做全局混合（在反归一化后，形状 [B, L, C]）
        if self.channel_mixer is not None:
            dec_out = self.channel_mixer(dec_out)  # [B, L, C]

        # Post-forecast STARBlock refinement
        if self.star_block is not None:
            dec_out = self.star_block(dec_out)

        return dec_out, attns


    def get_aux_loss(self):
        return getattr(self, "_aux_loss", 0.0)

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        dec_out, attns = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        if self.output_attention:
            return dec_out[:, -self.pred_len:, :], attns
        else:
            return dec_out[:, -self.pred_len:, :]  # [B, L, D]